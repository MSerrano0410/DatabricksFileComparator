{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd01f902-f4ac-4fdc-9b20-0d703b56e44b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Job needs the ability to iterate through two directories, housed in mounted folders in Databricks, and run both pair-wise and field-level comparisons between sets of two files (one modern and one legacy)\n",
    "\n",
    " legacy and modern directories : \n",
    " - 2 sets of output files \n",
    " - 1 from legacy code and 1 from modern code with output files to different S3 directories.\n",
    "\n",
    "\n",
    "- output directory\n",
    "- prefix/suffix\n",
    "- agency/segment/state code\n",
    "\n",
    "\n",
    "irmf_payee_state_41762\n",
    "\n",
    "For the other output files there are 3 types of reports: These report output files will not be loaded back into databricks. We want to only complete file-level comparison between related legacy and modern files.\n",
    "\n",
    "Report 1:   417-62-40.   will produce 1 report per segment (AA - AZ)\n",
    "file naming convention:\n",
    "\n",
    "I41766-F040.<SS>.L<YYYY><CC>\n",
    " \n",
    "Report 2:  417-63-40 will produce 1 report per segment (AA - AZ)\n",
    "file naming convention:\n",
    "\n",
    "I41767-F040.<SS>.L<YYYY><CC>\n",
    " \n",
    "Report 3 :  417-64-99  will produce 1 report per segment (AA-AZ)\n",
    "file naming convention:\n",
    "\n",
    "I41767-F099.<SS>.L<YYYY><CC>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60859e15-2b5f-42fd-8e12-940d0dea27d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TODO: Use difflib to have structured and unstructured comparisons. Unstructured is a string-by-string comparison using difflib, so we'd need a way to detect if a file is structured/unstructured and route accordingly for comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8564bb28-37e1-4e8e-9ba5-62437c07d5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"column1\", StringType(), True),\n",
    "    StructField(\"column2\", StringType(), True),\n",
    "    # Add more fields as per the actual CSV structure\n",
    "])\n",
    "\n",
    "\n",
    "# Read CSV files into DataFrames with the defined schema\n",
    "df_417_62_40 = spark.read.schema(schema).csv(\"/Volumes/labuser11156715_1755179084/default/i41766-f040/*.csv\")\n",
    "df_417_63_40 = spark.read.schema(schema).csv(\"/Volumes/labuser11156715_1755179084/default/i41767-f040/*.csv\")\n",
    "df_417_64_99 = spark.read.schema(schema).csv(\"/Volumes/labuser11156715_1755179084/default/i41767-f099/*.csv\")\n",
    "\n",
    "# Write DataFrames to tables with schema merging enabled\n",
    "df_417_62_40.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"labuser11156715_1755179084.default.irmf_payee_state_41766_f040\")\n",
    "df_417_63_40.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"labuser11156715_1755179084.default.irmf_payee_state_41767_f040\")\n",
    "df_417_64_99.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"labuser11156715_1755179084.default.irmf_payee_state_41767_f099\")\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Concatenate all columns into a single column with a table-like format\n",
    "df_417_62_40_concat = df_417_62_40.select(concat_ws(\" | \", *df_417_62_40.columns).alias(\"concatenated_row\"))\n",
    "\n",
    "display(df_417_62_40_concat)\n",
    "# Generate summary statistics for each DataFrame\n",
    "summary_417_62_40 = df_417_62_40.summary()\n",
    "summary_417_63_40 = df_417_63_40.summary()\n",
    "summary_417_64_99 = df_417_64_99.summary()\n",
    "\n",
    "# Display the summary statistics\n",
    "#display(summary_417_62_40)\n",
    "#display(summary_417_63_40)\n",
    "#display(summary_417_64_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7064998f-0f0d-4739-8c6f-b49275f587bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare rows in each summary DataFrame by joining on 'summary' column\n",
    "comparison_62_63 = summary_417_62_40.join(summary_417_63_40, on=\"summary\", how=\"inner\")\n",
    "comparison_63_99 = summary_417_63_40.join(summary_417_64_99, on=\"summary\", how=\"inner\")\n",
    "comparison_62_99 = summary_417_62_40.join(summary_417_64_99, on=\"summary\", how=\"inner\")\n",
    "\n",
    "display(comparison_62_63)\n",
    "display(comparison_63_99)\n",
    "display(comparison_62_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf19a1e-b12f-499e-bd8a-c8a534578b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create volume for 41766 text files\n",
    "spark.sql(\"\"\"\n",
    "DROP VOLUME IF EXISTS labuser11156715_1755179084.default.`I41767-F099_text`\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME labuser11156715_1755179084.default.`I41767-F099_text`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0768060-a0b6-4681-a142-10b908473940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "test_file = '/Volumes/labuser11156715_1755179084/default/i41767-f099_text/417-62_report.txt'\n",
    "compare_file = '/Volumes/labuser11156715_1755179084/default/i41767-f099_text/417-63_report.txt'\n",
    "\n",
    "with open(test_file, 'r') as f1,  open(compare_file, 'r') as f2:\n",
    "    file1_lines = f1.readlines()\n",
    "    file2_lines = f2.readlines()\n",
    "    match_bool = file1_lines == file2_lines\n",
    "    print(f\"Testfile: {test_file}\")\n",
    "    print(f\"Compare File: {compare_file}\")\n",
    "    print(f\"Matche = {match_bool}\")\n",
    "\n",
    "difference = difflib.unified_diff(file1_lines, file2_lines, fromfile = test_file, tofile = compare_file)\n",
    "i = 0\n",
    "for line in difference: \n",
    "   print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75bf70b6-64f6-450a-b137-4ef5e0934d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TODO: Make compare_dbfs iterative on directories, error handling to that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2de3df7-c3c2-4612-9a09-f80ed553193c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409e2bb8-142e-42e7-ae36-e659e3e2c9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def compare_dbfs_files_distributed(\n",
    "    file1_path: str,\n",
    "    file2_path: str,\n",
    "    output_csv_path: str = None,\n",
    "    show_differences: bool = True,\n",
    "    show_limit: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    Distributed comparison of two DBFS text files using Spark DataFrames.\n",
    "   \n",
    "    Args:\n",
    "        file1_path (str): Path to first file in DBFS (e.g., \"dbfs:/FileStore/...\")\n",
    "        file2_path (str): Path to second file in DBFS.\n",
    "        output_csv_path (str): Optional DBFS path to write mismatches as CSV (e.g. \"dbfs:/FileStore/comparison/mismatches.csv\")\n",
    "        show_differences (bool): If True, prints mismatches.\n",
    "        show_limit (int): Maximum number of mismatches to print.\n",
    "   \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame containing mismatched lines.\n",
    "    \"\"\"\n",
    "    # Read files as Spark DataFrames\n",
    "    df1 = spark.read.text(file1_path).withColumnRenamed(\"value\", \"line1\")\n",
    "    df2 = spark.read.text(file2_path).withColumnRenamed(\"value\", \"line2\")\n",
    "   \n",
    "    # Assign row numbers for proper alignment\n",
    "    w = Window.orderBy(F.monotonically_increasing_id())\n",
    "    df1 = df1.withColumn(\"line_no\", F.row_number().over(w))\n",
    "    df2 = df2.withColumn(\"line_no\", F.row_number().over(w))\n",
    "   \n",
    "   \n",
    "    # Full outer join by line number\n",
    "    joined = df1.join(df2, on=\"line_no\", how=\"outer\").orderBy(\"line_no\")\n",
    "   \n",
    "    # Find lines that differ or are missing\n",
    "    mismatches = joined.filter(\n",
    "        (F.col(\"line1\") != F.col(\"line2\")) | F.col(\"line1\").isNull() | F.col(\"line2\").isNull()\n",
    "    )\n",
    "   \n",
    "    mismatch_count = mismatches.count()\n",
    "    print(f\"File 1: {file1_path}\")\n",
    "    print(f\"File 2: {file2_path}\")\n",
    "    print(f\"Match = {mismatch_count == 0}\")\n",
    "   \n",
    "    # Print mismatches if requested\n",
    "    if mismatch_count > 0 and show_differences:\n",
    "        print(\"\\n--- Differences (showing up to\", show_limit, \"rows) ---\")\n",
    "        mismatches.show(show_limit, truncate=False)\n",
    "        print(\"\\n-------------------\")\n",
    "   \n",
    "    # Write mismatches to DBFS CSV if a path is given\n",
    "    if output_csv_path is not None and mismatch_count > 0:\n",
    "        mismatches.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_csv_path)\n",
    "        print(f\"Mismatches written to: {output_csv_path}\")\n",
    "   \n",
    "    return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e95ba687-fbe6-4663-9c64-574d800792b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_file = '/Volumes/labuser11156715_1755179084/default/i41767-f099_text/417-62_report.txt'\n",
    "compare_file = '/Volumes/labuser11156715_1755179084/default/i41767-f099_text/417-63_report.txt'\n",
    "csv_file = '/Volumes/labuser11156715_1755179084/default/file_compare_csv/output.csv'\n",
    "df_mismatches = compare_dbfs_files_distributed(test_file, compare_file, csv_file)\n",
    "display(df_mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a74e02e-d061-484d-b7ee-bf4653a3c630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def parse_with_difflib(file_path, schema):\n",
    "    df = spark.read.text(file_path)\n",
    "    \n",
    "    # Apply difflib parsing to each row in each column\n",
    "    def parse_row(row):\n",
    "        # Example: join all column values and get a unified diff with itself (placeholder logic)\n",
    "        text = \" \".join([str(x) for x in row])\n",
    "        diff = \"\\n\".join(difflib.unified_diff([text], [text]))\n",
    "        return diff\n",
    "\n",
    "    parse_udf = udf(parse_row, StringType())\n",
    "    df_parsed = df.withColumn(\"difflib_parsed\", parse_udf(*df.columns))\n",
    "    return df_parsed\n",
    "\n",
    "df_417_62_40 = parse_with_difflib(\"/Volumes/labuser11156715_1755179084/default/i41767-f099_text/*\", schema)\n",
    "df_417_63_40 = parse_with_difflib(\"/Volumes/labuser11156715_1755179084/default/i41767-f099_text/*\", schema)\n",
    "df_417_64_99 = parse_with_difflib(\"/Volumes/labuser11156715_1755179084/default/i41767-f099_text/*\", schema)\n",
    "\n",
    "display(df_417_62_40)\n",
    "display(df_417_63_40)\n",
    "display(df_417_64_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0967fe28-604d-4ec8-b454-dbc6aa1114ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE labuser11156715_1755179084.default.irmf_payee_state_41766_f040 \n",
    "-- CREATE OR REPLACE TABLE labuser11156715_1755179084.default.irmf_payee_state_41767_f040 \n",
    "--CREATE OR REPLACE TABLE labuser11156715_1755179084.default.irmf_payee_state_41767_f099\n",
    "(\n",
    "        separate_cd varchar(3),\n",
    "        valid_ind varchar(1),\n",
    "        primary_tin varchar(9),\n",
    "        student_ind varchar(1),\n",
    "        dist_code_1099k varchar(2),\n",
    "        merch_category_cd varchar(4),\n",
    "        student_ind_grad varchar(1),\n",
    "        reserve2 varchar(1),\n",
    "        ind1099k varchar(1),\n",
    "        document_code varchar(2),\n",
    "        payer_tin varchar(9),\n",
    "        payer_tin_type varchar(1),\n",
    "        amended_indicator varchar(1),\n",
    "        tax_year varchar(4),\n",
    "        payment_cd_1 varchar(3),\n",
    "        payment_sign_1 varchar(1),\n",
    "        payment_amt_1 varchar(11),\n",
    "        payment_cd_2 varchar(3),\n",
    "        payment_sign_2 varchar(1),\n",
    "        payment_amt_2 varchar(11),\n",
    "        payment_cd_3 varchar(3),\n",
    "        payment_sign_3 varchar(1),\n",
    "        payment_amt_3 varchar(11),\n",
    "        payment_cd_4 varchar(3),\n",
    "        payment_sign_4 varchar(1),\n",
    "        payment_amt_4 varchar(11),\n",
    "        payment_cd_5 varchar(3),\n",
    "        payment_sign_5 varchar(1),\n",
    "        payment_amt_5 varchar(11),\n",
    "        payment_cd_6 varchar(3),\n",
    "        payment_sign_6 varchar(1),\n",
    "        payment_amt_6 varchar(11),\n",
    "        payment_cd_7 varchar(3),\n",
    "        payment_sign_7 varchar(1),\n",
    "        payment_amt_7 varchar(11),\n",
    "        payment_cd_8 varchar(3),\n",
    "        payment_sign_8 varchar(1),\n",
    "        payment_amt_8 varchar(11),\n",
    "        payment_cd_9 varchar(3),\n",
    "        payment_sign_9 varchar(1),\n",
    "        payment_amt_9 varchar(11),\n",
    "        payment_cd_10 varchar(3),\n",
    "        payment_sign_10 varchar(1),\n",
    "        payment_amt_10 varchar(11),\n",
    "        payment_cd_11 varchar(3),\n",
    "        payment_sign_11 varchar(1),\n",
    "        payment_amt_11 varchar(11),\n",
    "        payment_cd_12 varchar(3),\n",
    "        payment_sign_12 varchar(1),\n",
    "        payment_amt_12 varchar(11),\n",
    "        payment_cd_13 varchar(3),\n",
    "        payment_sign_13 varchar(1),\n",
    "        payment_amt_13 varchar(11),\n",
    "        payer_acct_num varchar(20),\n",
    "        perfection_history varchar(2),\n",
    "        change_tin_ind varchar(1),\n",
    "        chng_nm_ctrl_ind varchar(1),\n",
    "        payee_name_line_1 varchar(50),\n",
    "        payee_name_line_2 varchar(40),\n",
    "        payee_address varchar(40),\n",
    "        payee_city_state varchar(40),\n",
    "        payee_zip_cd varchar(9),\n",
    "        payer_name_line_1 varchar(40),\n",
    "        payer_name_line_2 varchar(40),\n",
    "        payer_address_line varchar(40),\n",
    "        payer_city_state_zip varchar(40),\n",
    "        direct_sales varchar(1),\n",
    "        gambling_cd varchar(1),\n",
    "        item_desc varchar(38),\n",
    "        bond_cusip varchar(13),\n",
    "        job_id varchar(50),\n",
    "        source_file_nm varchar(50)\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4bdecc-8f53-4047-add1-ec3378bd86ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6036708260824852,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "File Comparison (Final) (2)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
